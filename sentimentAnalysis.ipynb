{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.ml import Pipeline\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "from delta import *\n",
    "\n",
    "# Import Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Sentiment Analysis\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    #.master(\"spark://namenode:7077\")\\\n",
    "    #.config(\"spark.executor.instances\", \"1\")\\\n",
    "    #.config(\"spark.executor.cores\", \"2\")\n",
    "\n",
    "    \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/temp/filtered_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|WI2cpA5VSEgGE5Pl9...|pym7c6ZFEtmoH16xN...|We saw the review...|2019-10-04 11:50:29|Restaurants, Bars...|  4.0|\n",
      "|JMm_2beO-LpbNc4r6...|Ei5HBqe012ImhqEr2...|This is a delight...|2019-09-08 21:52:26|Italian, Restaura...|  5.0|\n",
      "|bWk_7CwRfQpdkZMMM...|UmjITdXHhEF46ho6I...|This place is FUC...|2019-04-14 04:48:15|Adult Entertainme...|  1.0|\n",
      "|bBqyHGJpbjp68gmmP...|hy5GpGXAna-5qrb3z...|Literally disgust...|2019-05-20 22:54:11|Mexican, Restaura...|  1.0|\n",
      "|SH_DWH_hzRBTc2TRK...|vN6v8m4DO45Z4pp8y...|Surrey's is great...|2013-01-07 02:00:05|Vegetarian, Resta...|  4.0|\n",
      "|eT1QJGwbjVbRiElo4...|g_nLH7QGP3_l1eE-7...|Oh how yummy and ...|2017-08-11 01:34:23|Seafood, Specialt...|  5.0|\n",
      "|FwcvIMPuP70kmHuTv...|gLaxSkjWzt25hybka...|Food was good spe...|2017-04-23 18:59:19|Diners, Cajun\\/Cr...|  4.0|\n",
      "|zqIp_35IpN9rc9F16...|1ig2AJg8A08_XA3pF...|My husband and I ...|2020-03-10 01:31:35|Steakhouses, Seaf...|  5.0|\n",
      "|JBIZlWB8JHq2pTw0b...|4fQPIhCh2bUJBZcxQ...|It has worth Food...|2019-03-08 00:33:00|Buffets, Restaura...|  1.0|\n",
      "|I3y9xywZ_d44JH4uZ...|FHNIvNgh3fS7VZQq2...|I was very underw...|2020-03-03 12:58:55|Asian Fusion, Tap...|  2.0|\n",
      "|P8M8oyimYd2KRY8AN...|ju4YP8SLdR_BmWr_-...|We enjoyed our ph...|2019-04-17 00:32:41|Food, Vietnamese,...|  4.0|\n",
      "|51nXy_OpWO7kYvC4Y...|qONvIS94nZhXYm-jc...|Not wearing masks...|2021-02-18 18:22:11|Restaurants, Barb...|  4.0|\n",
      "|T95H_I82xgdBy4mhR...|V1n-wIrt5xtI0VIAx...|Cajun, Cajun and ...|2021-04-25 19:25:55|Seafood, Cajun\\/C...|  5.0|\n",
      "|7j5XG6wmSOU4NvHRm...|368cyArdo0P0aDUOh...|I have to stay aw...|2021-04-26 14:36:59|Sandwiches, Food,...|  5.0|\n",
      "|4IeDjxdTwxyM_l30f...|ZapDvyE7lJxVxlmee...|That shrimp po bo...|2021-04-22 23:52:59|Breakfast & Brunc...|  4.0|\n",
      "|JWqAFyJ_2Cw8TzpAU...|-K0zTgGyxo-AeSkcV...|\"I have not been ...|2019-02-22 14:02:40|Mediterranean, Re...|  5.0|\n",
      "|6lxqZ0_18xhc5LAr2...|ZH-D4qbLggBSvwpF0...|Really wish I did...|2018-11-07 03:48:29|Breakfast & Brunc...|  2.0|\n",
      "|Qkgb4Y-PrkYnYrqvI...|jmwasbZfgj3honf79...|Awesome tacos!!!!...|2018-11-15 03:03:37|Mexican, Bars, Re...|  5.0|\n",
      "|SupX6BNAxopez1FZU...|CA5BOxKRDPGJgdUQ8...|The updated marke...|2013-12-27 18:02:12|Grocery, Burgers,...|  5.0|\n",
      "|38DmZvrvDVuBjUxi5...|Sh5x88Ty9NGgMvLoI...|We were referred ...|2018-10-20 18:22:08|Bars, Nightlife, ...|  5.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline\n",
    "## Create a document assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\\\n",
    "    .fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ | ]sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "vivekn = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols(\"final_sentiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data as a Spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([\n",
    "    [\"I recommend this movie\"],\n",
    "    [\"Dont waste your time!!!\"]\n",
    "]).toDF(\"text\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the pipeline to the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|final_sentiment|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+\n",
      "|WI2cpA5VSEgGE5Pl9...|pym7c6ZFEtmoH16xN...|We saw the review...|2019-10-04 11:50:29|Restaurants, Bars...|  4.0|     [negative]|\n",
      "|JMm_2beO-LpbNc4r6...|Ei5HBqe012ImhqEr2...|This is a delight...|2019-09-08 21:52:26|Italian, Restaura...|  5.0|     [negative]|\n",
      "|bWk_7CwRfQpdkZMMM...|UmjITdXHhEF46ho6I...|This place is FUC...|2019-04-14 04:48:15|Adult Entertainme...|  1.0|     [positive]|\n",
      "|bBqyHGJpbjp68gmmP...|hy5GpGXAna-5qrb3z...|Literally disgust...|2019-05-20 22:54:11|Mexican, Restaura...|  1.0|     [positive]|\n",
      "|SH_DWH_hzRBTc2TRK...|vN6v8m4DO45Z4pp8y...|Surrey's is great...|2013-01-07 02:00:05|Vegetarian, Resta...|  4.0|     [negative]|\n",
      "|eT1QJGwbjVbRiElo4...|g_nLH7QGP3_l1eE-7...|Oh how yummy and ...|2017-08-11 01:34:23|Seafood, Specialt...|  5.0|     [positive]|\n",
      "|FwcvIMPuP70kmHuTv...|gLaxSkjWzt25hybka...|Food was good spe...|2017-04-23 18:59:19|Diners, Cajun\\/Cr...|  4.0|     [positive]|\n",
      "|zqIp_35IpN9rc9F16...|1ig2AJg8A08_XA3pF...|My husband and I ...|2020-03-10 01:31:35|Steakhouses, Seaf...|  5.0|     [positive]|\n",
      "|JBIZlWB8JHq2pTw0b...|4fQPIhCh2bUJBZcxQ...|It has worth Food...|2019-03-08 00:33:00|Buffets, Restaura...|  1.0|     [negative]|\n",
      "|I3y9xywZ_d44JH4uZ...|FHNIvNgh3fS7VZQq2...|I was very underw...|2020-03-03 12:58:55|Asian Fusion, Tap...|  2.0|     [negative]|\n",
      "|P8M8oyimYd2KRY8AN...|ju4YP8SLdR_BmWr_-...|We enjoyed our ph...|2019-04-17 00:32:41|Food, Vietnamese,...|  4.0|     [positive]|\n",
      "|51nXy_OpWO7kYvC4Y...|qONvIS94nZhXYm-jc...|Not wearing masks...|2021-02-18 18:22:11|Restaurants, Barb...|  4.0|     [negative]|\n",
      "|T95H_I82xgdBy4mhR...|V1n-wIrt5xtI0VIAx...|Cajun, Cajun and ...|2021-04-25 19:25:55|Seafood, Cajun\\/C...|  5.0|     [positive]|\n",
      "|7j5XG6wmSOU4NvHRm...|368cyArdo0P0aDUOh...|I have to stay aw...|2021-04-26 14:36:59|Sandwiches, Food,...|  5.0|     [negative]|\n",
      "|4IeDjxdTwxyM_l30f...|ZapDvyE7lJxVxlmee...|That shrimp po bo...|2021-04-22 23:52:59|Breakfast & Brunc...|  4.0|     [positive]|\n",
      "|JWqAFyJ_2Cw8TzpAU...|-K0zTgGyxo-AeSkcV...|\"I have not been ...|2019-02-22 14:02:40|Mediterranean, Re...|  5.0|     [negative]|\n",
      "|6lxqZ0_18xhc5LAr2...|ZH-D4qbLggBSvwpF0...|Really wish I did...|2018-11-07 03:48:29|Breakfast & Brunc...|  2.0|     [positive]|\n",
      "|Qkgb4Y-PrkYnYrqvI...|jmwasbZfgj3honf79...|Awesome tacos!!!!...|2018-11-15 03:03:37|Mexican, Bars, Re...|  5.0|     [positive]|\n",
      "|SupX6BNAxopez1FZU...|CA5BOxKRDPGJgdUQ8...|The updated marke...|2013-12-27 18:02:12|Grocery, Burgers,...|  5.0|     [positive]|\n",
      "|38DmZvrvDVuBjUxi5...|Sh5x88Ty9NGgMvLoI...|We were referred ...|2018-10-20 18:22:08|Bars, Nightlife, ...|  5.0|     [negative]|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mright_prediction\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                    F\u001b[39m.\u001b[39mwhen(((F\u001b[39m.\u001b[39marray_contains(F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mfinal_sentiment\u001b[39m\u001b[39m\"\u001b[39m),\u001b[39m\"\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mstars\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misin([\u001b[39m\"\u001b[39m\u001b[39m5.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m4.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m3.0\u001b[39m\u001b[39m\"\u001b[39m]))) \u001b[39m|\u001b[39m\n\u001b[1;32m      3\u001b[0m                         ((F\u001b[39m.\u001b[39marray_contains(F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mfinal_sentiment\u001b[39m\u001b[39m\"\u001b[39m),\u001b[39m\"\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m&\u001b[39m (F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mstars\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misin([\u001b[39m\"\u001b[39m\u001b[39m3.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1.0\u001b[39m\u001b[39m\"\u001b[39m]))), \n\u001b[1;32m      4\u001b[0m                         \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39motherwise(\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result = result.withColumn(\"right_prediction\", \n",
    "                   F.when(((F.array_contains(F.col(\"final_sentiment\"),\"positive\")) & (F.col(\"stars\").isin([\"5.0\", \"4.0\", \"3.0\"]))) |\n",
    "                        ((F.array_contains(F.col(\"final_sentiment\"),\"negative\")) & (F.col(\"stars\").isin([\"3.0\", \"2.0\", \"1.0\"]))), \n",
    "                        1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 18) / 20]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m count_ones \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49magg(F\u001b[39m.\u001b[39;49msum(\u001b[39m\"\u001b[39;49m\u001b[39mright_prediction\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39;49mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mcollectToPython()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_ones = result.agg(F.sum(\"right_prediction\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227628"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6831727932118494"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ones / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(review_id='p8Tlu_gaub7KoBEleGg2fA', business_id='f8ss-XDpkdwwbQf0-V6JCA', text='Very authentic Chinese food, and great happy hour specials. Dined here probably around 10 times and already looking forward to going back. Standouts include crab ragoon, the whole fried fish, garlic clams and their pea tips.', date='2020-09-20 20:23:35', categories='Cantonese, Restaurants, Nightlife, Cajun\\\\/Creole, Asian Fusion, Seafood, Chinese, Bars, Wine Bars', stars='4.0', final_sentiment=['negative'], right_prediction=0),\n",
       " Row(review_id='_ZNA4BU4HPke6N-g1tmuyA', business_id='X5pLH_HQG0ckmsqkjDxEHA', text='Giving 4 stars only because the whole grouper dinner meal was undercooked! However, service, tuna trio, fish tacos and the kids meal were all great!', date='2021-08-11 20:56:19', categories='American (Traditional), Nightlife, Bars, Restaurants, Seafood', stars='4.0', final_sentiment=['negative'], right_prediction=0),\n",
       " Row(review_id='qw29d27PzPs66Aqk2UghFA', business_id='uR7G8I4Cef9D9R340TN24Q', text='Okay, so being quite inebriated I can say without a shadow of a doubt.  Chili pepper shrimp at Trenasse is above and beyond.  Best shrimp in NOLA.Update: Still delicious.', date='2020-02-22 17:51:47', categories='American (New), American (Traditional), Southern, Taiwanese, Seafood, Bars, Nightlife, Cocktail Bars, Live\\\\/Raw Food, Wine Bars, Cajun\\\\/Creole, Restaurants', stars='5.0', final_sentiment=['negative'], right_prediction=0),\n",
       " Row(review_id='-5wWwbWHieQlynV83OIOvg', business_id='EY5cTfLf9BNG3tgoez9fKg', text=\"This place is so damn good. I saw them profiled on the Bay News 9 early this morning and I went over there and ate the Mofango...I hope I'm spelling that right...the one I had was the veggie one and that coconut sauce is to die for!!I'm not gonna lie, it made love to my soul!!!I said it...there u are!!My thing for 2020 is food is the new orgasm!!\", date='2020-05-06 19:00:04', categories='Caribbean, Puerto Rican, Latin American, Restaurants', stars='5.0', final_sentiment=['negative'], right_prediction=0),\n",
       " Row(review_id='urDo5UQZJlplnIU_YHjRGg', business_id='9Wq455RePLf5wbp4C4noQA', text=\"Friendly service and tasty food! Very friendly and cool atmosphere! Ask for Lee, he's the boss of all bosses!!  Spicy MiSo\", date='2020-02-05 19:25:52', categories='Restaurants, Japanese, Ramen, Food, Japanese Curry, Salad, Desserts', stars='5.0', final_sentiment=['negative'], right_prediction=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(F.col(\"right_prediction\") == 0).take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve this by training own viveken model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF.withColumn(\"train_sentiment\", F.when(F.col(\"stars\").isin([\"5.0\", \"4.0\"]), \"positive\")\n",
    "                                      .when(F.col(\"stars\").isin([\"1.0\", \"2.0\"]), \"negative\")\n",
    "                                      .otherwise(None))  # set to None for rows with 3.0\n",
    "\n",
    "# drop rows with 3.0\n",
    "trainDF = trainDF.filter(F.col(\"stars\") != \"3.0\")\n",
    "testDF = testDF.filter(F.col(\"stars\") != \"3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vivekn= (\n",
    "    ViveknSentimentApproach()\n",
    "    .setInputCols([\"document\", \"normal\"])\n",
    "    .setOutputCol(\"result_sentiment\")\n",
    "    .setSentimentCol(\"train_sentiment\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline().setStages([document, token, normalizer, train_vivekn, finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|--2KOIf3Rg7qDqpaY...|1Vo4BLw75ntATAJHY...|\"Great view, that...|2014-07-30 18:25:20|Restaurants, Musi...|  1.0|\n",
      "|--4INAzazK6omgf3m...|vz0oI7GR9AOtmJpO5...|Although my pork ...|2011-06-02 19:47:41|Irish, Pubs, Amer...|  4.0|\n",
      "|--4fzr2yfPWIvZP9X...|ZAnLKdfQhX0pj3cwo...|I am not sure if ...|2015-07-19 12:59:20|Breakfast & Brunc...|  1.0|\n",
      "|--DogY-DxnpGI6HgI...|IWHdx0NhDKADkGOgX...|First time I went...|2012-01-17 22:37:38|Restaurants, Amer...|  5.0|\n",
      "|--Esto4G8tuwgW1ZM...|tsOEt8v3chHwL1rZi...|Located less than...|2018-03-04 21:00:55|Vietnamese, Resta...|  4.0|\n",
      "|--KsMF-3s2D0Lxwu8...|uwoXUyqzKAiiM6DW1...|Go somewhere else...|2014-07-14 14:50:56|Sports Bars, Amer...|  1.0|\n",
      "|--RquisWmBzcezXZr...|kMfWeZIv6g5J1VoBZ...|Friend  treated m...|2013-10-13 04:52:17|Restaurants, Brea...|  4.0|\n",
      "|--Zf_DpkRen_zvjtO...|hAbyDqwRiULAua64I...|this place was be...|2012-11-27 16:54:13|Restaurants, Amer...|  1.0|\n",
      "|--_QglyOA15lGv7Ym...|y4VFp6qk_EFgNy9yK...|Amazing food, exc...|2018-04-19 01:55:23|Pizza, Mediterran...|  5.0|\n",
      "|--_ZmzhCxhFYgERwZ...|1RUStfReoIm_8NzWX...|Hamachi ceviche, ...|2017-09-23 16:34:59|Restaurants, Nigh...|  5.0|\n",
      "|--bra4W_9WsPdglwW...|-T_lkOvaK39R-Ufg6...|This place redefi...|2016-01-30 23:59:19|Desserts, Food, C...|  5.0|\n",
      "|--e8XpIAhcr5ZeRMj...|Cz-2ekKVyKheFccKt...|This place is ama...|2017-09-23 14:25:34|Food, Restaurants...|  5.0|\n",
      "|--llwLGZVO0ZPhete...|hExi86DTBlmIhB2FL...|\"Love this restau...|2017-10-27 20:35:32|Desserts, Restaur...|  4.0|\n",
      "|--ppOSCySbfJgDlaY...|Jzb2IQSwPfULWzdl9...|This is a dream! ...|2016-02-29 20:08:43|Soul Food, Food, ...|  5.0|\n",
      "|--xkthqXkg2NIKbde...|RuPk-ScXo4ycpdnAk...|I guess this plac...|2017-07-10 23:03:58|Restaurants, Sush...|  5.0|\n",
      "|-0Ou9EfJR6QreTJa7...|_r8Uv8fZpkTqQBaaG...|Love this place. ...|2018-05-16 20:45:06|Nightlife, Restau...|  5.0|\n",
      "|-0ceRSBz93SvUrfqC...|PtgpBXbgBiBwEbaGH...|One star for the ...|2013-02-19 17:57:04|Nightlife, Restau...|  1.0|\n",
      "|-0gQSFXt63GH6YTEX...|Md4vk4NBbbt_jfR-f...|Another new low f...|2016-04-24 04:04:11|Restaurants, Beer...|  1.0|\n",
      "|-0hU0GOzoyB0RWtCw...|yb-534-JqStRt0HmH...|One of my worst f...|2016-07-17 00:27:48|Sandwiches, Wine ...|  1.0|\n",
      "|-0hfzeDPrYKqPilXE...|YsgNELwtqulRa3l3a...|\"Considering my r...|2012-02-18 04:56:37|Restaurants, Paki...|  2.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 18) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:28:57.273 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 2 on 192.168.1.138: Executor heartbeat timed out after 135563 ms\n",
      "12:28:57.332 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 1 on 192.168.1.192: Executor heartbeat timed out after 139810 ms\n",
      "12:28:57.334 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 0 on 192.168.1.59: Executor heartbeat timed out after 141939 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 18) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:29:21.167 [rpc-server-4-1] ERROR org.apache.spark.network.server.TransportRequestHandler - Error while invoking RpcHandler#receive() on RPC id 7591011196615457716\n",
      "java.io.OptionalDataException: null\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_362]\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$readObject$1(HashMap.scala:195) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashTable.init(HashTable.scala:110) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashTable.init$(HashTable.scala:89) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashMap.init(HashMap.scala:44) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashMap.readObject(HashMap.scala:195) ~[scala-library-2.12.15.jar:?]\n",
      "\tat sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source) ~[?:?]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2322) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_362]\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) ~[netty-handler-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]\n",
      "12:29:21.943 [rpc-server-4-2] ERROR org.apache.spark.network.server.TransportRequestHandler - Error while invoking RpcHandler#receive() on RPC id 7390177481162374448\n",
      "java.io.OptionalDataException: null\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1692) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_362]\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$readObject$1(HashMap.scala:195) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashTable.init(HashTable.scala:110) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashTable.init$(HashTable.scala:89) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashMap.init(HashMap.scala:44) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.HashMap.readObject(HashMap.scala:195) ~[scala-library-2.12.15.jar:?]\n",
      "\tat sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source) ~[?:?]\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_362]\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2322) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2119) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1657) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2431) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2355) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2213) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1669) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503) ~[?:1.8.0_362]\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_362]\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:123) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:646) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:697) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:682) ~[spark-core_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) ~[netty-handler-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 18) / 20]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[39m=\u001b[39m training_pipeline\u001b[39m.\u001b[39;49mfit(trainDF)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:31:57.223 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 5 on 192.168.1.59: Executor heartbeat timed out after 173899 ms\n",
      "12:31:57.240 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 4 on 192.168.1.192: Executor heartbeat timed out after 173842 ms\n",
      "12:31:57.248 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 3 on 192.168.1.138: Executor heartbeat timed out after 162549 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                       (0 + 18) / 20]\r"
     ]
    }
   ],
   "source": [
    "pipeline = training_pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m trianin_pipeline\u001b[39m.\u001b[39;49mtransform(testDF)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "result = pipeline.transform(testDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:19:06.830330: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:19:13.974786: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 15:19:16.305972: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m document_assembler \u001b[39m=\u001b[39m DocumentAssembler() \\\n\u001b[1;32m      2\u001b[0m \u001b[39m.\u001b[39msetInputCol(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m use \u001b[39m=\u001b[39m UniversalSentenceEncoder\u001b[39m.\u001b[39;49mpretrained(\u001b[39m'\u001b[39;49m\u001b[39mtfhub_use\u001b[39;49m\u001b[39m'\u001b[39;49m, lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      6\u001b[0m \u001b[39m.\u001b[39msetInputCols([\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m])\\\n\u001b[1;32m      7\u001b[0m \u001b[39m.\u001b[39msetOutputCol(\u001b[39m\"\u001b[39m\u001b[39msentence_embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# classifier = SentimentDLModel().pretrained('sentimentdl_use_twitter')\\\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#     .setInputCols([\"sentence_embeddings\"])\\\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#     .setOutputCol(\"sentiment\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m#l_model = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\")))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/annotator/embeddings/universal_sentence_encoder.py:211\u001b[0m, in \u001b[0;36mUniversalSentenceEncoder.pretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads and loads a pretrained model.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39m    The restored model\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msparknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpretrained\u001b[39;00m \u001b[39mimport\u001b[39;00m ResourceDownloader\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m ResourceDownloader\u001b[39m.\u001b[39;49mdownloadModel(UniversalSentenceEncoder, name, lang, remote_loc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/pretrained/resource_downloader.py:96\u001b[0m, in \u001b[0;36mResourceDownloader.downloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     94\u001b[0m t1\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     j_obj \u001b[39m=\u001b[39m _internal\u001b[39m.\u001b[39;49m_DownloadModel(reader\u001b[39m.\u001b[39;49mname, name, language, remote_loc, j_dwn)\u001b[39m.\u001b[39mapply()\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/__init__.py:338\u001b[0m, in \u001b[0;36m_DownloadModel.__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, reader, name, language, remote_loc, validator):\n\u001b[0;32m--> 338\u001b[0m     \u001b[39msuper\u001b[39;49m(_DownloadModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp.pretrained.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m validator \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.downloadModel\u001b[39;49m\u001b[39m\"\u001b[39;49m, reader,\n\u001b[1;32m    339\u001b[0m                                          name, language, remote_loc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39msuper\u001b[39m(ExtendedJavaWrapper, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(java_obj)\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_java_obj(java_obj, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_java_obj\u001b[39m(\u001b[39mself\u001b[39m, java_class, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(java_class, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39;49mjava_args)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol(\"text\") \\\n",
    ".setOutputCol(\"document\")\n",
    "\n",
    "use = UniversalSentenceEncoder.pretrained('tfhub_use', lang=\"en\") \\\n",
    ".setInputCols([\"document\"])\\\n",
    ".setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# classifier = SentimentDLModel().pretrained('sentimentdl_use_twitter')\\\n",
    "#     .setInputCols([\"sentence_embeddings\"])\\\n",
    "#     .setOutputCol(\"sentiment\")\n",
    "\n",
    "#nlp_pipeline = Pipeline(stages=[document_assembler,use,classifier])\n",
    "\n",
    "#l_model = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SentimentDLModel().pretrained('sentimentdl_use_twitter')\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler,use,classifier])\n",
    "\n",
    "l_model = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m annotations \u001b[39m=\u001b[39m l_model\u001b[39m.\u001b[39mfullAnnotate([\u001b[39m\"\u001b[39m\u001b[39mim meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis upset that he can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt update his Facebook by texting it... and might cry as a result  School today also. Blah!\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l_model' is not defined"
     ]
    }
   ],
   "source": [
    "annotations = l_model.fullAnnotate([\"im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\", \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies_sentiment_analysis download started this may take some time.\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[39m=\u001b[39m PretrainedPipeline(\u001b[39m\"\u001b[39;49m\u001b[39mmovies_sentiment_analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m result \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mannotate(\u001b[39m\"\"\"\u001b[39m\u001b[39mI love johnsnowlabs!  \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/pretrained/pretrained_pipeline.py:54\u001b[0m, in \u001b[0;36mPretrainedPipeline.__init__\u001b[0;34m(self, name, lang, remote_loc, parse_embeddings, disk_location)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m, remote_loc\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_embeddings\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, disk_location\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m disk_location:\n\u001b[0;32m---> 54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ResourceDownloader()\u001b[39m.\u001b[39;49mdownloadPipeline(name, lang, remote_loc)\n\u001b[1;32m     55\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m PipelineModel\u001b[39m.\u001b[39mload(disk_location)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/pretrained/resource_downloader.py:138\u001b[0m, in \u001b[0;36mResourceDownloader.downloadPipeline\u001b[0;34m(name, language, remote_loc)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Downloads and loads a pipeline with the default downloader.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m    The loaded pipeline\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mprint\u001b[39m(name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m download started this may take some time.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 138\u001b[0m file_size \u001b[39m=\u001b[39m _internal\u001b[39m.\u001b[39;49m_GetResourceSize(name, language, remote_loc)\u001b[39m.\u001b[39mapply()\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m file_size \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCan not find the model to download please check the name!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/__init__.py:379\u001b[0m, in \u001b[0;36m_GetResourceSize.__init__\u001b[0;34m(self, name, language, remote_loc)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, language, remote_loc):\n\u001b[0;32m--> 379\u001b[0m     \u001b[39msuper\u001b[39;49m(_GetResourceSize, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    380\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mcom.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.getDownloadSize\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, language, remote_loc)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:27\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39msuper\u001b[39m(ExtendedJavaWrapper, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(java_obj)\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_java_obj(java_obj, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sparknlp/internal/extended_java_wrapper.py:37\u001b[0m, in \u001b[0;36mExtendedJavaWrapper.new_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_java_obj\u001b[39m(\u001b[39mself\u001b[39m, java_class, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_java_obj(java_class, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[39m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m java_class\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[39m=\u001b[39m [_py2java(sc, arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m java_obj(\u001b[39m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1709\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1707\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1709\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1710\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1711\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1712\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1713\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1714\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline(\"movies_sentiment_analysis\", \"en\")\n",
    "\n",
    "result = pipeline.annotate(\"\"\"I love johnsnowlabs!  \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
