{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-03-24 08:14:52,698 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# used findpark because python did not know where our spark installation was. \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "spark = (SparkSession.builder.appName(\"shopping\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "|           review_id|             user_id|         business_id|stars|useful|funny|cool|                text|               date|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "|MmmrqbdupYBJq9jnl...|TjD2S66AWxlKsPSQB...|znK6tMeegKf9YnGut...|  5.0|     0|    0|   0|Great little spot...|2021-04-16 22:02:21|\n",
      "|1_uWX0bBJNnZjPEcp...|Vtfxxq3nxdYHRiH6O...|UxwpCVLgPWCeaRyet...|  5.0|     0|    0|   0|James was super k...|2021-01-03 21:54:48|\n",
      "|WVAEE47MnM3Sv2cvM...|ZDw_qN5Fy6PE0gdbV...|eVI64EQymywsvMLmD...|  5.0|     0|    0|   0|By far the best i...|2020-11-01 19:50:03|\n",
      "|D5b6iooZZcJ8nG-9P...|UvMDlX2wV4Md9OwGW...|SPcPJfPgWzhjUDqVF...|  5.0|     0|    0|   0|I thought that th...|2021-03-24 05:53:49|\n",
      "|uDxnT7zWFnxEDxp5e...|ir9ixBZPzBwWe9IZ_...|T5XzQ6YnVExvd0BOR...|  5.0|     0|    0|   0|Aside from the re...|2016-06-13 21:25:10|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema = [\"business_id STRING ,name STRING, address STRING, city STRING,\"state\":\"CA\",\"postal_code\":\"93101\",\"latitude\":34.4266787,\"longitude\":-119.7111968,\"stars\":5.0,\"review_count\":7,\"is_open\":0,\"attributes\":{\"ByAppointmentOnly\":\"True\"},\"categories\":\"Doctors, Traditional Chinese Medicine, Naturopathic\\/Holistic, Acupuncture, Health & Medical, Nutritionists\",\"hours\":null}\"]\n",
    "from pyspark.sql.types import *\n",
    "review_schema = StructType([StructField(\"review_id\", StringType(), False),\n",
    "      StructField(\"user_id\", StringType(), False),\n",
    "      StructField(\"business_id\", StringType(), False),\n",
    "      StructField(\"stars\", StringType(), False), \n",
    "      StructField(\"useful\", IntegerType(), False),\n",
    "      StructField(\"funny\", IntegerType(), False),\n",
    "      StructField(\"cool\", IntegerType(), False),\n",
    "      StructField(\"text\", StringType(), False),\n",
    "      StructField(\"date\", StringType(), False),])\n",
    "review_df = spark.read.csv(\"hdfs://namenode:9000/project_data/review.csv\", sep = '|', header = False, schema = review_schema)\n",
    "review_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DONT RUN THIS AGAIN\n",
    "review_df.write.mode(\"overwrite\").saveAsTable('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken = (review_df\n",
    "          .where(F.col(\"business_id\").isNull())\n",
    ")\n",
    "# broken = (df\n",
    "#           .where(F.col(\"review_id\").contains('One small area of')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (14 + 1) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----------+-----+------+-----+----+----+----+\n",
      "|           review_id|user_id|business_id|stars|useful|funny|cool|text|date|\n",
      "+--------------------+-------+-----------+-----+------+-----+----+----+----+\n",
      "|ys8wYr2V++++failed\\t|   null|       null| null|  null| null|null|null|null|\n",
      "|jfVm-Qep++++failed\\t|   null|       null| null|  null| null|null|null|null|\n",
      "|m74VYMFn++++failed\\t|   null|       null| null|  null| null|null|null|null|\n",
      "+--------------------+-------+-----------+-----+------+-----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "broken.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "parquet_table = \"review_table_parquet\" # name of the table \n",
    "review_df.write.format(\"parquet\").saveAsTable(parquet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(review_id='6fObpwIggOQR1oDapyQzyg', user_id='lN-1uUHeV_QyFbczw8ErlA', business_id='grpNey31cTGKrhmQQaNGdA', stars='5.0', useful=0, funny=0, cool=0, text='Had a wonderful, authentic basque dinner Friday night.  Service was great and friendly and the bar was fun. Thanks Gavin!  We will definitely be back!Chris and Cathy (Turlock CA)', date='2020-06-28 19:45:32')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.table(\"review_table_parquet\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS review_table_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "|         business_id|                name|             address|        city|state|postal_code|     latitude|     longitude|stars|review_count|is_open|          attributes|          categories|               hours|\n",
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "|O4dxdtqo625JFN0uF...|Kelly's Bake Shop...|        1582 Main St|     Dunedin|   FL|      34698|    28.019895|   -82.7610784|  4.5|          49|      0|\"{'BusinessParkin...|Sandwiches, Resta...|{'Tuesday': '7:0-...|\n",
      "|Q0k9AkPe9PWfGNvhd...|            3D Vapor|   105 E Vandalia St|Edwardsville|   IL|      62025|   38.8115422|    -89.955539|  3.5|           8|      1|\"{'BikeParking': ...|Food, Shopping, V...|{'Monday': '10:0-...|\n",
      "|Ol4KW8Xx4lzPitSOw...|             Chili's|   16420 State Rd 54|      Odessa|   FL|      33556|28.1882190617|-82.5451382756|  2.5|          66|      1|\"{'BusinessAccept...|Cocktail Bars, Re...|{'Monday': '11:0-...|\n",
      "|iN-dxbDqVDHmbYlT-...|North Riverfront ...|Biddle St & N Leo...| Saint Louis|   MO|      63102|   38.6351743|   -90.1814549|  3.5|          16|      1|\"{'GoodForKids': ...|Hiking, Bike Rent...|                null|\n",
      "|5BpQb0e4i9gw0HUjO...|Bucks County Free...|    1080 Edgewood Rd|     Yardley|   PA|      19067|    40.224761|   -74.8420319|  3.5|           5|      1|                null|Education, Educat...|                null|\n",
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", StringType(), True)\n",
    "])\n",
    "business_df = spark.read.csv(\"hdfs://namenode:9000/project_data/business.csv\", sep = '|', header = False, schema = business_schema)\n",
    "business_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN THIS AGAIN\n",
    "business_df.write.mode(\"overwrite\").saveAsTable('business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_business = (business_df\n",
    "          .where(F.col(\"business_id\").isNull() | F.col(\"categories\").isNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+----+-----+-----------+--------+---------+-----+------------+-------+----------+----------+-----+\n",
      "|business_id|name|address|city|state|postal_code|latitude|longitude|stars|review_count|is_open|attributes|categories|hours|\n",
      "+-----------+----+-------+----+-----+-----------+--------+---------+-----+------------+-------+----------+----------+-----+\n",
      "+-----------+----+-------+----+-----+-----------+--------+---------+-----+------------+-------+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "broken_business.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining two tables\n",
    "combined_table = spark.sql(\"SELECT text FROM reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- review_count: integer (nullable = true)\n",
      " |-- is_open: integer (nullable = true)\n",
      " |-- attributes: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- hours: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business=spark.read.parquet('hdfs://namenode:9000/project_data/data/business/')\n",
    "business.printSchema()\n",
    "\n",
    "# TODO how to read parquet in sql\n",
    "# combined_table = spark.sql(\"SELECT name FROM business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# create temporary table to use sql \n",
    "business.createOrReplaceTempView(\"business\")\n",
    "print(type(business))\n",
    "btest = spark.sql(\"SELECT name FROM business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150346"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews =spark.read.parquet('hdfs://namenode:9000/project_data/data/reviews/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "# TODO what is suitable ON or WHERE??? (ON is the suitable operation)\n",
    "combined_table = spark.sql(\"SELECT r.text, b.business_id, b.categories FROM reviews AS r LEFT JOIN business AS b ON b.business_id = r.business_id \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table_1 = combined_table.where(F.col('categories').contains(\"Restaurants\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4724468"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_table_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m findspark\u001b[39m.\u001b[39minit()\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      7\u001b[0m builder \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mMyApp\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.extensions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mio.delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.catalog.spark_catalog\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m spark \u001b[39m=\u001b[39m configure_spark_with_delta_pip(builder)\u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'delta'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "|         business_id|                name|             address|        city|state|postal_code|     latitude|     longitude|stars|review_count|is_open|          attributes|          categories|               hours|\n",
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "|O4dxdtqo625JFN0uF...|Kelly's Bake Shop...|        1582 Main St|     Dunedin|   FL|      34698|    28.019895|   -82.7610784|  4.5|          49|      0|\"{'BusinessParkin...|Sandwiches, Resta...|{'Tuesday': '7:0-...|\n",
      "|Q0k9AkPe9PWfGNvhd...|            3D Vapor|   105 E Vandalia St|Edwardsville|   IL|      62025|   38.8115422|    -89.955539|  3.5|           8|      1|\"{'BikeParking': ...|Food, Shopping, V...|{'Monday': '10:0-...|\n",
      "|Ol4KW8Xx4lzPitSOw...|             Chili's|   16420 State Rd 54|      Odessa|   FL|      33556|28.1882190617|-82.5451382756|  2.5|          66|      1|\"{'BusinessAccept...|Cocktail Bars, Re...|{'Monday': '11:0-...|\n",
      "|iN-dxbDqVDHmbYlT-...|North Riverfront ...|Biddle St & N Leo...| Saint Louis|   MO|      63102|   38.6351743|   -90.1814549|  3.5|          16|      1|\"{'GoodForKids': ...|Hiking, Bike Rent...|                null|\n",
      "|5BpQb0e4i9gw0HUjO...|Bucks County Free...|    1080 Edgewood Rd|     Yardley|   PA|      19067|    40.224761|   -74.8420319|  3.5|           5|      1|                null|Education, Educat...|                null|\n",
      "+--------------------+--------------------+--------------------+------------+-----+-----------+-------------+--------------+-----+------------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", StringType(), True)\n",
    "])\n",
    "business_df = spark.read.csv(\"hdfs://namenode:9000/project_data/business.csv\", sep = '|', header = False, schema = business_schema)\n",
    "business_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9c0b034e-6912-49f8-b913-d902a90955db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 290ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9c0b034e-6912-49f8-b913-d902a90955db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/28ms)\n",
      "2023-03-24 08:37:15,308 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"spark://namenode:7077\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.load.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.internal.SQLConf$.PARQUET_FIELD_ID_READ_ENABLED()Lorg/apache/spark/internal/config/ConfigEntry;\n\tat io.delta.sql.DeltaSparkSessionExtension.$anonfun$apply$3(DeltaSparkSessionExtension.scala:88)\n\tat org.apache.spark.sql.SparkSessionExtensions.$anonfun$buildResolutionRules$1(SparkSessionExtensions.scala:174)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.SparkSessionExtensions.buildResolutionRules(SparkSessionExtensions.scala:174)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.customResolutionRules(BaseSessionStateBuilder.scala:212)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1.<init>(BaseSessionStateBuilder.scala:187)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.analyzer(BaseSessionStateBuilder.scala:179)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$2(BaseSessionStateBuilder.scala:357)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:87)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:87)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:440)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m business\u001b[39m=\u001b[39mspark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mparquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mhdfs://namenode:9000/project_data/data/business/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    159\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.load.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.internal.SQLConf$.PARQUET_FIELD_ID_READ_ENABLED()Lorg/apache/spark/internal/config/ConfigEntry;\n\tat io.delta.sql.DeltaSparkSessionExtension.$anonfun$apply$3(DeltaSparkSessionExtension.scala:88)\n\tat org.apache.spark.sql.SparkSessionExtensions.$anonfun$buildResolutionRules$1(SparkSessionExtensions.scala:174)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.SparkSessionExtensions.buildResolutionRules(SparkSessionExtensions.scala:174)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.customResolutionRules(BaseSessionStateBuilder.scala:212)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder$$anon$1.<init>(BaseSessionStateBuilder.scala:187)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.analyzer(BaseSessionStateBuilder.scala:179)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$build$2(BaseSessionStateBuilder.scala:357)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:87)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:87)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:440)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "business=spark.read.format(\"parquet\").load('hdfs://namenode:9000/project_data/data/business/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
