{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-880150c4-2232-4122-ab35-9d86e93e1b84;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 240ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-880150c4-2232-4122-ab35-9d86e93e1b84\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "from delta import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"spark://namenode:7077\")\\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 0) / 2]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mdelta\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/temp/filtered_reviews\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    178\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/temp/filtered_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|WI2cpA5VSEgGE5Pl9...|pym7c6ZFEtmoH16xN...|We saw the review...|2019-10-04 11:50:29|Restaurants, Bars...|  4.0|\n",
      "|JMm_2beO-LpbNc4r6...|Ei5HBqe012ImhqEr2...|This is a delight...|2019-09-08 21:52:26|Italian, Restaura...|  5.0|\n",
      "|bWk_7CwRfQpdkZMMM...|UmjITdXHhEF46ho6I...|This place is FUC...|2019-04-14 04:48:15|Adult Entertainme...|  1.0|\n",
      "|bBqyHGJpbjp68gmmP...|hy5GpGXAna-5qrb3z...|Literally disgust...|2019-05-20 22:54:11|Mexican, Restaura...|  1.0|\n",
      "|SH_DWH_hzRBTc2TRK...|vN6v8m4DO45Z4pp8y...|Surrey's is great...|2013-01-07 02:00:05|Vegetarian, Resta...|  4.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Links\n",
    "First lets see if links are even used in Yelp reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(review_id='FAjTzZf7ifNbJxp2yhTgLA', business_id='OM5beWRkxZSu1TShUC3EDw', text='It was a tasty treat indeed. I ate it as recommended by Trevor with the bib and everything. The saucy was super butter 9000 and I would totally eat that this joint again for sure. Even now my fingers and bread smell of the glorious butter deliciousness. Here is my live yelp https:\\\\/\\\\/www.youtube.com\\\\/watch?v=9dmSN5IfU9Y', date='2017-10-03 03:40:46', categories='Food, Specialty Food, Seafood, Cajun\\\\/Creole, Restaurants', stars='5.0'),\n",
       " Row(review_id='DGnzpZlqRdzNQ3rgBRE9Ng', business_id='AueGCAS9RtM2RU19euf49w', text=\"We have our weekly family (usually Saturday night's) outing at Giuseppe's, here in Lambertville, New Jersey, and Joanne and the staff always provide a pleasant place for us to gather - with their BYOB policy, we enjoy a festive time, excellent background music (usually a nice mix of old classics), and of-course great pizza (from white pizza, vegetarian pizza, and regular red pizza) with all the toppings you want. Also, my special favorite dinner is the Eggplant Parmesan - For vegetarians, try the eggplant on a roll - See some of the dishes in the below pictures via my Flickr page:https:\\\\/\\\\/www.flickr.com\\\\/photos\\\\/mvm33\\\\/11514863576\\\\/in\\\\/set-72157624976422586https:\\\\/\\\\/www.flickr.com\\\\/photos\\\\/mvm33\\\\/11356856406\\\\/in\\\\/set-72157624976422586\", date='2015-04-02 17:08:18', categories='Pizza, Italian, Restaurants, Salad', stars='5.0'),\n",
       " Row(review_id='t3U5wv9vYfjlpPuYCmkkmQ', business_id='OkyiiyLbPCM0HVGp3jps2Q', text=\"As a favor to yourself get the salsa because it is seriously unbelievable.This place is amazing. The food is always fresh and look at my pic to see how much fresh avocado comes with it! Plus this was to-go. You know a place is great when their to-go food is superb.I'm from California and we pride ourselves on the quality of Mexican food offered and this is place can definitely compete. Plus it's a family restaurant!!https:\\\\/\\\\/s3-media1.fl.yelpcdn.com\\\\/bphoto\\\\/bD425ehaz7-_DXwN18xvFg\\\\/168s.jpg\", date='2018-07-27 18:01:19', categories='Health Markets, Food, Specialty Food, Restaurants, Mexican', stars='5.0'),\n",
       " Row(review_id='PjKNyowb4yZ0Femqr8Ip5g', business_id='oCoiJ-GBriiupCgpgWfVhQ', text='\"Be sure to check out my video of this trip (and many others) in addition to reading this review. You can see first hand the places I review in video form! https:\\\\/\\\\/www.youtube.com\\\\/watch?v=p_H2NeSdyvsHad to visit this place after watching episode after episode of \"It\\'s Always Sunny in Philadelphia\" on my trip. This pub was located just down from my hostel in the old section of the city. The pub itself was nice. A lot smaller than I thought it was going to be. The service was good and the drinks were fairly priced. People were friendly and took our pic inside and out front for us.I think it\\'s worth the stop in just to be \"part of the show\" but other than that its just a bar. There was no band or anything other than drinking to bring you to the bar. If you\\'re looking for games or a band try other bars in town - as this is not the place for that.\"', date='2016-12-05 20:29:40', categories='Restaurants, American (Traditional), Nightlife, Bars, Pubs', stars='3.0'),\n",
       " Row(review_id='9uRbDj8P9uVYfLxcxtOtDQ', business_id='punLF4oSWHbH7JK4wO4HzA', text=\"There's just something about sushi.  Perhaps it's the raw protein; maybe it's the exotic flavors, or it could just be the anticipation of eating a new and unique creation.  Despite the particular draw, I know that I love it.  As a hearty eater, my love for sushi runs deeper when the food never runs out.  Your Humble Writer has been pleased to have made the acquaintance of There's just something about sushi.  Perhaps it's the draw of eating raw fish, or maybe it's the unique combination of flavors found in rolls, but, for whatever reason, it rocks!  Sushi rocks even harder when the food is never ending, fresh, and not dried out on a buffet.   Sound good?  Welcome to Watami Sushi All You Can Eat.I previously wrote a detailed review of Watami Sushi's flagship restaurant, which is located in Lawrence.  Since the vast majority of the Broad Ripple's location's menu items are the same, I am not going to re-review them.  The earlier review can be read here: https:\\\\/\\\\/www.yelp.com\\\\/biz\\\\/watami-sushi-all-you-can-eat-indianapolis?hrid=peAaWHGsiyqMxeVGcgiQsQ&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct).While the menus at the Lawrence and Broad Ripple locations are almost the same, they are not identical.  The Lawrence location offers a wider variety of non-sushi items.  One of my particular favorites, chicken udon, is not offered in Broad Ripple.  I found that disappointing.  According to the manager, the reason that this delectable dish is not offered is due to the small size of the Broad Ripple location's kitchen.  However, Broad Ripple has a larger sushi offering, and it has daily specials.  Unfortunately, I did not have the opportunity to sample any of them - as I didn't notice them written on the small chalkboards placed throughout the restaurant.  Having our server mention them, for, you know, the seemly oblivious customer (like me), would have been nice.Despite having fewer non-sushi choices, there was no shortage of great food, including what I consider the best-of-the-best-of-the-best: the 465 Roll.  Just order several of them; trust me!  Honorable mentions go to the Spider Roll, the Volcano, and the Sweet Potato Temaki.  For me, I am sure to start every meal at Watami with a round of piping hot Crab Rangoon.  Of course, any Asian feast is well ended with a scoop or two of green tea ice cream.Watami's Broad Ripple location lacks the character and ambiance of the Lawrence restaurant.  The owners could use an interior decorator.  While I thought the manager was quite polite and helpful, I found our server impolite and impatient.  While our first course was expediently delivered, the second, third, and fourth courses seemed purposefully delayed.  Upon ordering our second or third round, she was sure to mention that we would incur and extra charge for any uneaten food (which was partially debunked by the manager).  I'm not a large man, but I'm the president of the Clean Plate Club.Watami Sushi All You Can Eat in Broad Ripple: the name is a mouthful, but that is exactly what the customer will receive when eating here.  Overall, the place is a winner.  I'll be back.\", date='2017-03-29 21:46:02', categories='Sushi Bars, Restaurants, Japanese, Salad', stars='4.0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(F.col('text').contains('https')).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn('text', F.regexp_replace('text', r'http[s]?:\\S+|www\\.[^\\s]*', '')) #remove links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the links are gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(review_id='dRjRdepVvaSZM7bJEGifcw', business_id='peeXG1Ra-DClPQv14jvKvg', text=\"Just had my first big Chinese food craving since moving to Philly. Lucky for me I had you yelpers to help me pick the perfect spot to satisfy that need.I ordered online, which was easy and comfortable. Only weird thing was that when I went to pay, there wasn't a secure (https) link where you enter your credit card info. I'm paranoid, so cash it was.I was impressed at how fast the food came. It was less than 30 min after I submitted my order that a nice delivery person was knocking on my door in the art museum area. Given that it was a rainy Saturday night, I was pleased with the timing.The food arrived hot -- not lame-o lukewarm like some delivery places. The portions were large and the food was tasty. We always order way (I mean WAY) more than necessary so we tried and enjoyed all of the following:pot stickers (chicken, steamed, and very tender), beef with broccoli (crunchy broccoli - yay!!) (asked for brown rice and we got it), veggie lo mein (average), chicken fried rice (a little bland but that's the nature of the beast I think), and hot and sour soup (good stuff). My husband and I have left-overs for about 4 more yummy meals.I will definitely order from Square on Square again. Especially since according to my fortune, I am the master of every situation!!!\", date='2009-06-14 02:57:45', categories='Seafood, Chinese, Restaurants, Tex-Mex, Asian Fusion, Sushi Bars, American (New), American (Traditional), Vegetarian, Italian', stars='4.0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.where(F.col('text').contains('https')).take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text contains not a link but a mention of the https protocol."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how one of the examples above looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_id='FAjTzZf7ifNbJxp2yhTgLA', business_id='OM5beWRkxZSu1TShUC3EDw', text='It was a tasty treat indeed. I ate it as recommended by Trevor with the bib and everything. The saucy was super butter 9000 and I would totally eat that this joint again for sure. Even now my fingers and bread smell of the glorious butter deliciousness. Here is my live yelp ', date='2017-10-03 03:40:46', categories='Food, Specialty Food, Seafood, Cajun\\\\/Creole, Restaurants', stars='5.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.where(F.col('review_id').contains('FAjTzZf7ifNbJxp2yhTgLA')).take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before: '... Here is my live yelp https:\\\\/\\\\/www.youtube.com\\\\/watch?v=9dmSN5IfU9Y'\n",
    "\n",
    "After: '... Here is my live yelp '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------------------+----------+--------------------+------------------+\n",
      "|summary|           review_id|         business_id|                  text|      date|          categories|             stars|\n",
      "+-------+--------------------+--------------------+----------------------+----------+--------------------+------------------+\n",
      "|  count|             4724468|             4724468|               4724468|   4724376|             4724468|           4724468|\n",
      "|   mean|                null|                null|                  null|      null|                null|3.7937981588614846|\n",
      "| stddev|                null|                null|                  null|      null|                null|1.3912277728543867|\n",
      "|    min|---4VcQZzy_vIIifU...|---kPU91CF4Lq2-Wl...|                      |          |Acai Bowls, Ameri...|               1.0|\n",
      "|    25%|                null|                null|                  null|      null|                null|               3.0|\n",
      "|    50%|                null|                null|                  null|      null|                null|               4.0|\n",
      "|    75%|                null|                null|                  null|      null|                null|               5.0|\n",
      "|    max|zzzz1ADBqBEVyfX4l...|zzyx5x0Z7xXWWvWnZ...|＼(^o^)／They have ...|  $13.99  |Yoga, Food, Cafes...|               5.0|\n",
      "+-------+--------------------+--------------------+----------------------+----------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedStarsDf = df2.withColumn('stars', F.col('stars').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|WI2cpA5VSEgGE5Pl9...|pym7c6ZFEtmoH16xN...|We saw the review...|2019-10-04 11:50:29|Restaurants, Bars...|  4.0|\n",
      "|JMm_2beO-LpbNc4r6...|Ei5HBqe012ImhqEr2...|This is a delight...|2019-09-08 21:52:26|Italian, Restaura...|  5.0|\n",
      "|bWk_7CwRfQpdkZMMM...|UmjITdXHhEF46ho6I...|This place is FUC...|2019-04-14 04:48:15|Adult Entertainme...|  1.0|\n",
      "|bBqyHGJpbjp68gmmP...|hy5GpGXAna-5qrb3z...|Literally disgust...|2019-05-20 22:54:11|Mexican, Restaura...|  1.0|\n",
      "|SH_DWH_hzRBTc2TRK...|vN6v8m4DO45Z4pp8y...|Surrey's is great...|2013-01-07 02:00:05|Vegetarian, Resta...|  4.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixedStarsDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====================================================>  (19 + 1) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3780240 rows in the training set, and 944228 in the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = fixedStarsDf.randomSplit([.8, .2], seed=42)\n",
    "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column text is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m VectorAssembler\n\u001b[1;32m      3\u001b[0m vecAssembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m vecTrainDF \u001b[39m=\u001b[39m vecAssembler\u001b[39m.\u001b[39;49mtransform(trainDF)\n\u001b[1;32m      7\u001b[0m vecTrainDF\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstars\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mshow(\u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[1;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:400\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type string of column text is not supported."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"text\"], outputCol=\"features\")\n",
    "\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "\n",
    "vecTrainDF.select(\"text\", \"features\", \"stars\").show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lets check that classes are similarly distributed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are moslty positive reviews with 5 and 4 stars. 1, 2 and 3 star reviewes make up only 1/2 of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fractions = df2.select(\"stars\").distinct().withColumn(\"fraction\", F.lit(0.8)).rdd.collectAsMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.0': 0.8, '5.0': 0.8, '4.0': 0.8, '2.0': 0.8, '3.0': 0.8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df2.sampleBy('stars', fractions, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df2.subtract(train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the same amount of data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|5KBe-TUWtawFwuew4...|uV7Zc9zIgDG4t2O_t...|We order the same...|2017-06-28 22:01:04|Mexican, Restaura...|  5.0|\n",
      "|rrNpGRFH2xQ2Pajo7...|H2Kulc2EWVO0kFmAD...|Ordered 'The Spic...|2016-05-20 03:16:25|Sandwiches, Resta...|  5.0|\n",
      "|3S1_0XZIJIpmOlDBX...|ejDrtuWQRGY0a3zjd...|\"This was my 5th ...|2013-05-30 18:14:20|Nightlife, Americ...|  2.0|\n",
      "|hNEUx4fOE-Aq_J9OQ...|eQHk2EMRtkYZR6DH-...|\"Yelp's phrase fo...|2012-01-06 17:14:57|American (New), B...|  2.0|\n",
      "|LwQJHKnbtje3LGCCc...|NJaR_2hke7NNAC6l1...|Super friendly st...|2016-05-08 21:50:20|Restaurants, Mexican|  3.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the distribution of star values in the training dataset, we can see that we have way more 5 star reviews than for example 2 star reviews. If we would like to get an even distribution we can compute a new facor as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = train.groupBy(\"stars\").count().orderBy('stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "|  1.0| 453317|\n",
      "|  2.0| 323645|\n",
      "|  3.0| 434598|\n",
      "|  4.0| 903966|\n",
      "|  5.0|1663173|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distribution.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = distribution.select(F.min(F.col(\"count\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323645"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = distribution.withColumn(\"count_ratio\", min_value / F.col(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_fractions = distribution.select('stars', 'count_ratio').rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_train = train.sampleBy('stars', even_fractions, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 178:============================================>          (16 + 4) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|stars| count|\n",
      "+-----+------+\n",
      "|  1.0|323507|\n",
      "|  2.0|323645|\n",
      "|  3.0|323635|\n",
      "|  4.0|324104|\n",
      "|  5.0|324124|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "even_train.groupBy(\"stars\").count().orderBy('stars').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the possible star values have around the same probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:============================================>          (16 + 4) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|stars| count|\n",
      "+-----+------+\n",
      "|  1.0|323507|\n",
      "|  2.0|323645|\n",
      "|  3.0|323635|\n",
      "|  4.0|324104|\n",
      "|  5.0|324124|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distribution = test.groupBy(\"stars\").count().orderBy('stars')\n",
    "min_value = distribution.select(F.min(F.col(\"count\"))).collect()[0][0]\n",
    "distribution = distribution.withColumn(\"count_ratio\", min_value / F.col(\"count\"))\n",
    "even_fractions = distribution.select('stars', 'count_ratio').rdd.collectAsMap()\n",
    "even_test = test.sampleBy('stars', even_fractions, seed = 1)\n",
    "even_train.groupBy(\"stars\").count().orderBy('stars').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "swr = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"filtered_words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"stars\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, swr, hashtf, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "wofiltered_wordsrds does not exist. Available: review_id, business_id, text, date, categories, stars, words, filtered_words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipelineFit \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(even_train)\n\u001b[1;32m      2\u001b[0m train_df \u001b[39m=\u001b[39m pipelineFit\u001b[39m.\u001b[39mtransform(even_train)\n\u001b[1;32m      3\u001b[0m val_df \u001b[39m=\u001b[39m pipelineFit\u001b[39m.\u001b[39mtransform(even_test)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py:112\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stage, Transformer):\n\u001b[1;32m    111\u001b[0m     transformers\u001b[39m.\u001b[39mappend(stage)\n\u001b[0;32m--> 112\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mfit(dataset)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:217\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_transform(dataset)\n\u001b[1;32m    216\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(dataset)\n\u001b[1;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be a param map but got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:354\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mtransform(dataset\u001b[39m.\u001b[39;49m_jdf), dataset\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: wofiltered_wordsrds does not exist. Available: review_id, business_id, text, date, categories, stars, words, filtered_words"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(even_train)\n",
    "train_df = pipelineFit.transform(even_train)\n",
    "val_df = pipelineFit.transform(even_test)\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:09:26,734 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1099.3 KiB\n",
      "2023-03-31 11:10:20,040 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.5 KiB\n",
      "2023-03-31 11:10:20,333 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:18,668 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:19,138 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:21,816 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:22,140 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:24,296 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:24,540 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:26,498 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:27,275 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:29,132 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:29,441 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:31,229 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:31,519 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:33,423 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:33,699 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:35,567 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:35,818 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:37,729 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:38,021 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:39,873 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:40,149 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:42,137 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:42,580 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:44,512 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:44,790 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:46,761 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:47,080 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:49,046 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:49,269 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:51,222 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:51,501 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:53,445 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:53,704 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:55,547 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:56,575 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:11:58,432 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:11:58,882 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:00,671 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:00,956 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:02,837 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:03,103 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:04,996 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:05,330 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:07,270 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:07,553 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:09,463 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:09,643 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:11,602 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:11,897 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:13,856 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:14,137 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:16,080 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:16,366 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:18,337 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:18,993 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:20,957 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:21,181 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:23,169 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:23,416 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:26,287 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:26,525 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:28,480 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:28,781 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:30,714 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:31,001 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:32,967 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:33,308 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:35,353 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:35,633 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:37,576 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:37,862 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:39,589 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:39,900 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:41,846 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:42,076 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:43,899 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:44,338 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:46,175 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:46,496 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:48,460 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:48,748 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:50,653 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:50,930 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:52,881 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:53,162 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:55,187 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:55,428 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:57,370 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:57,644 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:12:59,559 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:12:59,797 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:01,638 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:01,939 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:03,951 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:04,313 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:06,144 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:06,420 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:08,339 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:08,727 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:10,649 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:10,950 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:12,785 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:13,043 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:14,897 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:15,184 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:17,010 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:17,246 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:19,079 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:19,344 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:21,208 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:21,463 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:23,134 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:23,810 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:25,611 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:25,892 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:27,821 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:28,077 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:29,937 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:30,214 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:32,088 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:32,365 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:34,278 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:34,558 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:36,402 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:36,681 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:38,698 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:38,893 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:40,879 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:41,207 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:43,119 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:43,409 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:45,336 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:45,617 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:47,555 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:47,818 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:49,856 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:50,628 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:52,570 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:52,934 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:54,806 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:55,113 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:57,029 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:57,295 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:13:59,133 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:13:59,396 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:01,239 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:01,563 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:03,330 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:03,619 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:05,405 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:05,729 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:07,642 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:07,877 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:09,740 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:10,019 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:11,952 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:12,231 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:14,209 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:14,439 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:16,434 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:16,796 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:18,678 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:18,948 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:20,834 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:21,128 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:23,009 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:23,360 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:25,147 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:25,474 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:27,324 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:27,580 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:29,317 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:29,571 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:31,456 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:31,717 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:33,652 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:33,935 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:35,868 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:36,071 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:38,010 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:38,299 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:40,137 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:40,403 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:42,176 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:42,468 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:44,434 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:44,727 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:46,514 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:46,744 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:48,690 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:48,979 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:51,049 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:51,312 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:53,299 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:53,592 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:55,487 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:55,777 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:57,696 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:57,898 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:14:59,732 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:14:59,995 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:01,841 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:02,113 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:04,027 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:04,382 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:06,187 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:06,636 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:08,434 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:09,547 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:11,440 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:11,750 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:13,631 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:13,921 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:15,882 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:16,145 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:18,142 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:18,394 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:20,293 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:20,559 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:22,477 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:22,710 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:24,688 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:24,950 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:26,894 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:27,169 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:29,061 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "2023-03-31 11:15:29,300 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1100.9 KiB\n",
      "2023-03-31 11:15:31,163 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1102.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100, class_weight = 'balanced')\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:42:57,120 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|               words|                  tf|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|1l2b8pEVjmbkfHA48...|u5_y_iOUMjbvqSwWX...|\"I went for Resta...|2008-02-23 06:52:12|Italian, Restaurants|  2.0|[\"i, went, for, r...|(65536,[239,308,3...|(65536,[239,308,3...|  2.0|[-0.0153883566597...|[0.09452295222193...|       4.0|\n",
      "|3rbxCxwleVSQFDhHd...|CRnqAB-ViEfQ0CQGO...|The bread is very...|2019-04-24 15:14:24|Bakeries, Restaur...|  2.0|[the, bread, is, ...|(65536,[830,4477,...|(65536,[830,4477,...|  2.0|[-2.1977140672459...|[0.00455156709053...|       3.0|\n",
      "|6MPGYBVeNWAhTKnnQ...|loD9IlH0q4stjff3N...|Honey and I wante...|2014-10-05 15:30:23|Restaurants, Mexi...|  3.0|[honey, and, i, w...|(65536,[308,1198,...|(65536,[308,1198,...|  3.0|[-0.1949537295776...|[0.03122395002487...|       3.0|\n",
      "|9OCK-F1VsxFGXR0y7...|YV83252nBml7BmQLd...|\"Stopped here for...|2014-06-19 12:43:07|American (Traditi...|  4.0|[\"stopped, here, ...|(65536,[329,1603,...|(65536,[329,1603,...|  1.0|[-0.0673531207381...|[0.01989579741522...|       1.0|\n",
      "|BVpupG9iQY8md4xlW...|dOOvB4HW-b0mWDpcv...|This is a really ...|2019-02-11 03:52:00|Nightlife, Bars, ...|  5.0|[this, is, a, rea...|(65536,[2495,5462...|(65536,[2495,5462...|  0.0|[2.03750266858894...|[0.51296666405438...|       0.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 11:45:20,492 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5837941674528926"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "evaluator.setLabelCol(\"label\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select('prediction','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 10:58:20,510 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       0.0|  2.0|\n",
      "|       3.0|  2.0|\n",
      "|       3.0|  3.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionAndLabels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc\u001b[39m.\u001b[39mparallelize(predictionAndLabels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.parallelize(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ctx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[39m=\u001b[39m MulticlassMetrics(predictionAndLabels)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/mllib/evaluation.py:268\u001b[0m, in \u001b[0;36mMulticlassMetrics.__init__\u001b[0;34m(self, predictionAndLabels)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, predictionAndLabels):\n\u001b[0;32m--> 268\u001b[0m     sc \u001b[39m=\u001b[39m predictionAndLabels\u001b[39m.\u001b[39;49mctx\n\u001b[1;32m    269\u001b[0m     sql_ctx \u001b[39m=\u001b[39m SQLContext\u001b[39m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m    270\u001b[0m     numCol \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(predictionAndLabels\u001b[39m.\u001b[39mfirst())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1659\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[39m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> 1659\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n\u001b[1;32m   1661\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   1662\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ctx'"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 10:46:16,703 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.write.format(\"delta\").mode(\"overwrite\").save(\"/temp/predicted_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 23:23:18,685 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1119.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|               words|                  tf|            features|label|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "|2BC1-gBhvs2Wota17...|zZ01WQlcpI1_n806W...|\"Almost too large...|2018-10-26 15:40:33|Restaurants, Amer...|  4.0|[\"almost, too, la...|(65536,[1198,1257...|(65536,[1198,1257...|  1.0|\n",
      "|2J2sfhgashGlI-VIE...|hS8z5yNV0QJQcKo1a...|I wrote a review ...|2019-02-27 14:32:03|Pubs, Pizza, Nigh...|  5.0|[i, wrote, a, rev...|(65536,[680,1198,...|(65536,[680,1198,...|  0.0|\n",
      "|2UQmzimc0yNAHEC4m...|6ajnOk0GcY9xbb5Oc...|I loved my meal a...|2014-05-15 23:10:28|Mediterranean, Re...|  5.0|[i, loved, my, me...|(65536,[1797,1880...|(65536,[1797,1880...|  0.0|\n",
      "|2gq5dMASR-OMraE9g...|thb9MKU1kOG_TZ0sn...|Very disappointin...|2016-12-18 18:38:36|Steakhouses, Wine...|  1.0|[very, disappoint...|(65536,[178,241,3...|(65536,[178,241,3...|  2.0|\n",
      "|3lHEt3PFBHTiqCDmz...|RnURR_OfKGejCsEsh...|\"There is a large...|2013-08-13 18:57:03|Restaurants, Bars...|  4.0|[\"there, is, a, l...|(65536,[1257,2325...|(65536,[1257,2325...|  1.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:======================================================(22 + -14) / 20]\r"
     ]
    }
   ],
   "source": [
    "val_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 23:26:52,791 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1121.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:======================================================(22 + -14) / 20]\r"
     ]
    }
   ],
   "source": [
    "prediction = lrModel.predict(val_df.head().features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:======================================================(22 + -14) / 20]\r"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 23:51:31,640 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1121.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Wow, I am surprised that there are no reviews of this restaurant on Yelp...in fact, it looks like I am the first to write one??!!!! Oh!! Now that\\'s pressure.Anyway, this will be an easy review so no pressure here. Antonella\\'s is probably one of the better Italian restaurants that I have ever tried.  Each soup, salad, sandwich and platter at this little Italian eatery is magnificent. The service is personal and friendly, and nowhere else can you get better food in a hurry for a great price. The portions are big, the selection is great, and the smell is intoxicating. Watch out for the lunch time rush...call ahead instead. Haven\\'t had a bad dish there yet...and they cater for any of your special occasions.My recommendations: Lentil Soup, Eggplant Parmesan, the \"Ogelsby\" hoagie .All hoagies and sandwiches are terrific...and I\\'ve tried almost all of them. So overall here are my final ratings...Food: 5 starsService: 5 starsAmbiance: 3 starsOverall: 5 starsGotta go now, my mouth is watering from thinking about their delicious sandwiches.\"'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:======================================================(22 + -14) / 20]\r"
     ]
    }
   ],
   "source": [
    "val_df.head().text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
