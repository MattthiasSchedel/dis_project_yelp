{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/WhatsApp Image 2023-04-27 at 10.52.43.jpg' />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/WhatsApp Image 2023-04-27 at 10.53.04.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-02fd3d59-f346-48e8-b8fb-2e6ed8366c90;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 471ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-02fd3d59-f346-48e8-b8fb-2e6ed8366c90\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "from delta import *\n",
    "\n",
    "# Import Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Sentiment Analysis - presentation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"spark://namenode:7077\")\\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    #.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    \n",
    "\n",
    "    \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataframes\n",
    "# Review data\n",
    "spark.sparkContext.setJobDescription('load review dataset')\n",
    "review_schema = StructType([StructField(\"review_id\", StringType(), False),\n",
    "      StructField(\"user_id\", StringType(), False),\n",
    "      StructField(\"business_id\", StringType(), False),\n",
    "      StructField(\"stars\", StringType(), False), \n",
    "      StructField(\"useful\", IntegerType(), False),\n",
    "      StructField(\"funny\", IntegerType(), False),\n",
    "      StructField(\"cool\", IntegerType(), False),\n",
    "      StructField(\"text\", StringType(), False),\n",
    "      StructField(\"date\", StringType(), False),])\n",
    "# review_df = spark.read.csv(\"hdfs://namenode:9000/project_data/review.csv\", sep = '|', header = False, schema = review_schema)\n",
    "review_df = spark.read.csv(\"hdfs://namenode:9000/project_data/review_small_a.csv\", sep = '|', header = False, schema = review_schema)\n",
    "review_df.createOrReplaceTempView(\"reviews\")\n",
    "#review_df.persist()\n",
    "\n",
    "#review_df.show()\n",
    "\n",
    "# Business data\n",
    "spark.sparkContext.setJobDescription('load review dataset')\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", StringType(), True)\n",
    "])\n",
    "business_df = spark.read.csv(\"hdfs://namenode:9000/project_data/business.csv\", sep = '|', header = False, schema = business_schema)\n",
    "business_df.createOrReplaceTempView(\"business\")\n",
    "#business_df.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimisation\n",
    "### Combine and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|lr1xBBahWP2ZOqGco...|rv_1S0yg-YtulZ6gG...|Great coffee shop...|2020-06-08 02:07:44|Breakfast & Brunc...|  5.0|\n",
      "|VqHhMjGJI_9bTcgl6...|8ikj9GaC2oEf_leis...|This was so incre...|2016-05-15 16:22:32|Diners, Restauran...|  5.0|\n",
      "|h_UAfe8YoIpN5OGiH...|elSnD7Vuxnlg5_9gT...|decent interior f...|2017-01-10 16:20:32|Restaurants, Pizz...|  2.0|\n",
      "|yYhliOOeTZXlV_ZWb...|kfpwO_cdVQdTh2_oM...|The American Sard...|2013-08-22 04:02:49|Restaurants, Bars...|  4.0|\n",
      "|4okNZR_Z3whwioH3i...|BxfvdHqETU8jWYUjx...|I love shake shac...|2012-08-05 12:19:57|Food, Hot Dogs, B...|  4.0|\n",
      "|0oCbH-K0tFa0Exbjl...|UakVMT3xrpbFB2pHd...|Well I finally ha...|2018-06-01 22:47:56|Bars, Nightlife, ...|  5.0|\n",
      "|BXVm_qLhx2h0F1Cf-...|Oun4NN-u5yiHIxDqt...|Donuts and fried ...|2013-11-07 23:50:33|Restaurants, Kore...|  5.0|\n",
      "|LAlPaYMAWx0ZUNnDY...|ArHq_uM3K7mxAiqwF...|\"Always FANTASTIC...|2015-11-10 16:20:49|Restaurants, Brea...|  5.0|\n",
      "|2ecMImprVq9UhB6Pe...|iUZEGx29miZObLd6_...|Wow! First off, I...|2009-11-20 16:54:17|Breakfast & Brunc...|  5.0|\n",
      "|PcdwS2FL3jnpXIwmI...|VQcCL9PiNL_wkGf-u...|Good space upstai...|2016-03-02 19:29:51|American (New), R...|  3.0|\n",
      "|GfbZu-2hg6WzG5b8v...|sJ7RwRN1Pd602KDXb...|Passed ten other ...|2021-07-26 20:51:21|Pretzels, Food, S...|  3.0|\n",
      "|TiiJfsNpPfQ5my6Uq...|3iSRhLUZfZSlLaQw-...|\"Never again.  I ...|2010-04-22 00:35:34|Sushi Bars, Night...|  1.0|\n",
      "|E0m3ni49ruwkccjZe...|pN3O2ZLRiSLPyCWCe...|\"America used to ...|2010-09-27 04:29:02|Restaurants, Amer...|  1.0|\n",
      "|cYTp9j-Rbmpij11dG...|q-zV08jt6U-q05SME...|\"We said \"Hello\" ...|2009-06-17 00:13:56|Sandwiches, Fashi...|  5.0|\n",
      "|ezwqc_Dj0FbnbJKjQ...|9tOw202PkO9efW8zS...|\"I normally do no...|2011-12-01 03:37:00|  Pizza, Restaurants|  1.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine data on common restaurant id\n",
    "spark.sparkContext.setJobGroup(\"combine datasets\", \"combine datasets\")\n",
    "reviews_with_category = spark.sql(\"SELECT r.review_id, b.business_id, r.text, r.date, b.categories, r.stars FROM reviews AS r LEFT JOIN business AS b ON b.business_id = r.business_id \")\n",
    "\n",
    "# Filter for only restaurant reviews\n",
    "spark.sparkContext.setJobGroup(\"filter reviews\", \"filter reviews\")\n",
    "restaurant_reviews = reviews_with_category.where(F.col('categories').contains(\"Restaurants\"))\n",
    "restaurant_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4724468"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Combine\n",
    "We expect this to be faster since the restaurant dataframe is first made smaller.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|lr1xBBahWP2ZOqGco...|rv_1S0yg-YtulZ6gG...|Great coffee shop...|2020-06-08 02:07:44|Breakfast & Brunc...|  5.0|\n",
      "|VqHhMjGJI_9bTcgl6...|8ikj9GaC2oEf_leis...|This was so incre...|2016-05-15 16:22:32|Diners, Restauran...|  5.0|\n",
      "|h_UAfe8YoIpN5OGiH...|elSnD7Vuxnlg5_9gT...|decent interior f...|2017-01-10 16:20:32|Restaurants, Pizz...|  2.0|\n",
      "|yYhliOOeTZXlV_ZWb...|kfpwO_cdVQdTh2_oM...|The American Sard...|2013-08-22 04:02:49|Restaurants, Bars...|  4.0|\n",
      "|4okNZR_Z3whwioH3i...|BxfvdHqETU8jWYUjx...|I love shake shac...|2012-08-05 12:19:57|Food, Hot Dogs, B...|  4.0|\n",
      "|0oCbH-K0tFa0Exbjl...|UakVMT3xrpbFB2pHd...|Well I finally ha...|2018-06-01 22:47:56|Bars, Nightlife, ...|  5.0|\n",
      "|BXVm_qLhx2h0F1Cf-...|Oun4NN-u5yiHIxDqt...|Donuts and fried ...|2013-11-07 23:50:33|Restaurants, Kore...|  5.0|\n",
      "|LAlPaYMAWx0ZUNnDY...|ArHq_uM3K7mxAiqwF...|\"Always FANTASTIC...|2015-11-10 16:20:49|Restaurants, Brea...|  5.0|\n",
      "|2ecMImprVq9UhB6Pe...|iUZEGx29miZObLd6_...|Wow! First off, I...|2009-11-20 16:54:17|Breakfast & Brunc...|  5.0|\n",
      "|PcdwS2FL3jnpXIwmI...|VQcCL9PiNL_wkGf-u...|Good space upstai...|2016-03-02 19:29:51|American (New), R...|  3.0|\n",
      "|GfbZu-2hg6WzG5b8v...|sJ7RwRN1Pd602KDXb...|Passed ten other ...|2021-07-26 20:51:21|Pretzels, Food, S...|  3.0|\n",
      "|TiiJfsNpPfQ5my6Uq...|3iSRhLUZfZSlLaQw-...|\"Never again.  I ...|2010-04-22 00:35:34|Sushi Bars, Night...|  1.0|\n",
      "|E0m3ni49ruwkccjZe...|pN3O2ZLRiSLPyCWCe...|\"America used to ...|2010-09-27 04:29:02|Restaurants, Amer...|  1.0|\n",
      "|cYTp9j-Rbmpij11dG...|q-zV08jt6U-q05SME...|\"We said \"Hello\" ...|2009-06-17 00:13:56|Sandwiches, Fashi...|  5.0|\n",
      "|ezwqc_Dj0FbnbJKjQ...|9tOw202PkO9efW8zS...|\"I normally do no...|2011-12-01 03:37:00|  Pizza, Restaurants|  1.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for only restaurant reviews\n",
    "spark.sparkContext.setJobDescription('filter reviews')\n",
    "business_df = business_df.where(F.col('categories').contains(\"Restaurants\"))\n",
    "business_df.createOrReplaceTempView(\"business\")\n",
    "\n",
    "# Combine data on common restaurant id\n",
    "spark.sparkContext.setJobDescription('combine datasets')\n",
    "restaurant_reviews = spark.sql(\"SELECT r.review_id, b.business_id, r.text, r.date, b.categories, r.stars FROM reviews AS r INNER JOIN business AS b ON b.business_id = r.business_id \")\n",
    "\n",
    "restaurant_reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4724468"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ | ]sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setJobDescription('pipeline builder')\n",
    "# Build a pipeline\n",
    "# Document Assembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -tokenizer')\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")#\\\n",
    "    #.fit(restaurant_reviews) # apperantly works without this\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -normalizer')\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -model')\n",
    "vivekn = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -model=finish')\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols(\"final_sentiment\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline executor')\n",
    "pipeline = Pipeline().setStages([documentAssembler, tokenizer, normalizer, vivekn, finisher])\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline fit')\n",
    "pipelineModel = pipeline.fit(restaurant_reviews)\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline transform')\n",
    "# Calculate sentiment for all restaurant reviews\n",
    "result = pipelineModel.transform(restaurant_reviews)\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -result check')\n",
    "# Check accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding Accuracy without persist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy for sentiment: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setJobDescription('count right ones')\n",
    "result = result.withColumn(\"right_prediction\", \n",
    "                   F.when(((F.array_contains(F.col(\"final_sentiment\"),\"positive\")) & (F.col(\"stars\").isin([\"5.0\", \"4.0\", \"3.0\"]))) |\n",
    "                        ((F.array_contains(F.col(\"final_sentiment\"),\"negative\")) & (F.col(\"stars\").isin([\"3.0\", \"2.0\", \"1.0\"]))), \n",
    "                        1).otherwise(0))\n",
    "\n",
    "count_ones = result.agg(F.sum(\"right_prediction\")).collect()[0][0]\n",
    "\n",
    "#result gets dropped from the memory!!!\n",
    "\n",
    "print(f\"Prediction accuracy for sentiment: {count_ones/total_count}\")\n",
    "result.write.format(\"delta\").mode(\"overwrite\").save(\"/temp/sentiment_predicted_restaurant_reviews\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/withoutpersist.png' />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding accuracy with persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy for sentiment: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "result.persist()\n",
    "\n",
    "spark.sparkContext.setJobDescription('count right ones')\n",
    "#### TO-DO --- try to optomise below line.\n",
    "result = result.withColumn(\"right_prediction\", \n",
    "                   F.when(((F.array_contains(F.col(\"final_sentiment\"),\"positive\")) & (F.col(\"stars\").isin([\"5.0\", \"4.0\", \"3.0\"]))) |\n",
    "                        ((F.array_contains(F.col(\"final_sentiment\"),\"negative\")) & (F.col(\"stars\").isin([\"3.0\", \"2.0\", \"1.0\"]))), \n",
    "                        1).otherwise(0))\n",
    "count_ones = result.agg(F.sum(\"right_prediction\")).collect()[0][0]\n",
    "\n",
    "total_count = result.count()\n",
    "print(f\"Prediction accuracy for sentiment: {count_ones/total_count}\")\n",
    "\n",
    "# result.write.format(\"delta\").mode(\"overwrite\").save(\"/temp/sentiment_predicted_restaurant_reviews\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/WhatsApp Image 2023-04-27 at 09.45.30.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsert \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating delta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:======================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# delta_table_path = \"/temp/sentiment_predicted_restaurant_reviews\"\n",
    "delta_table_path = \"/temp/upsert_presentation_test1\"\n",
    "# Check if the Delta table exists\n",
    "if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "    print(\"Updating delta\")\n",
    "    # If the Delta table exists, load it into a DataFrame\n",
    "    deltaTableAnalyzedReviews = DeltaTable.forPath(spark, delta_table_path)\n",
    "    deltaTableAnalyzedReviews.alias('old') \\\n",
    "        .merge(\n",
    "        result.alias('updates'),\n",
    "        'old.review_id = updates.review_id'\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"review_id\": \"updates.review_id\",\n",
    "      \"business_id\": \"updates.business_id\",\n",
    "      \"text\": \"updates.text\",\n",
    "      \"date\": \"updates.date\",\n",
    "      \"categories\": \"updates.categories\",\n",
    "      \"stars\":\"updates.stars\",\n",
    "      \"final_sentiment\": \"updates.final_sentiment\",\n",
    "      \"right_prediction\": \"updates.right_prediction\"\n",
    "    }\n",
    "  ) \\\n",
    "  .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"review_id\": \"updates.review_id\",\n",
    "      \"business_id\": \"updates.business_id\",\n",
    "      \"text\": \"updates.text\",\n",
    "      \"date\": \"updates.date\",\n",
    "      \"categories\": \"updates.categories\",\n",
    "      \"stars\":\"updates.stars\",\n",
    "      \"final_sentiment\": \"updates.final_sentiment\",\n",
    "      \"right_prediction\": \"updates.right_prediction\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n",
    "else:\n",
    "    # If the Delta table does not exist, create it\n",
    "    print(\"Creating delta\")\n",
    "    deltaTableAnalyzedReviews = result.write.format('delta').mode('overwrite').save(delta_table_path)\n",
    "    deltaTableAnalyzedReviews = DeltaTable.forPath(spark, delta_table_path)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+----------------+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|final_sentiment|right_prediction|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+----------------+\n",
      "|-KOxrHhTSayEk01XH...|CrP6JWXBmf_HyMnZJ...|Good place to eat...|2013-02-20 17:34:12|Restaurants, Burg...| null|     [positive]|               1|\n",
      "|0oCbH-K0tFa0Exbjl...|UakVMT3xrpbFB2pHd...|Well I finally ha...|2018-06-01 22:47:56|Bars, Nightlife, ...|  5.0|     [positive]|               1|\n",
      "|2ecMImprVq9UhB6Pe...|iUZEGx29miZObLd6_...|Wow! First off, I...|2009-11-20 16:54:17|Breakfast & Brunc...|  5.0|     [positive]|               1|\n",
      "|4F0rrBlJ2GIbWcNNW...|UakVMT3xrpbFB2pHd...|Great breakfast b...|2018-12-24 18:49:00|Bars, Nightlife, ...| null|     [positive]|               1|\n",
      "|4okNZR_Z3whwioH3i...|BxfvdHqETU8jWYUjx...|I love shake shac...|2012-08-05 12:19:57|Food, Hot Dogs, B...|  4.0|     [positive]|               1|\n",
      "|4pw-OuNdrzzZx1NDa...|-CbBGlrmddJsaruk6...|The food isn't go...|2019-10-13 01:52:33|Salad, Restaurant...| null|     [positive]|               0|\n",
      "|AZTGIy-E97N3qSlm7...|LaQgujKtAQ4yPSmqR...|Great burgers and...|2017-10-27 22:41:00|American (Traditi...| null|     [positive]|               1|\n",
      "|ApYU3mkJPVqpsrDd5...|LdECsE8lJS7v5GTFT...|We are here.and l...|2018-09-12 00:09:57|Seafood, Restaurants| null|     [positive]|               1|\n",
      "|BXVm_qLhx2h0F1Cf-...|Oun4NN-u5yiHIxDqt...|Donuts and fried ...|2013-11-07 23:50:33|Restaurants, Kore...|  5.0|     [positive]|               1|\n",
      "|DhxtQTbPgjg5DIGDX...|pym7c6ZFEtmoH16xN...|Awesome service a...|2011-12-14 21:53:19|Restaurants, Bars...| null|     [positive]|               1|\n",
      "|E0m3ni49ruwkccjZe...|pN3O2ZLRiSLPyCWCe...|\"America used to ...|2010-09-27 04:29:02|Restaurants, Amer...|  1.0|     [negative]|               1|\n",
      "|GfbZu-2hg6WzG5b8v...|sJ7RwRN1Pd602KDXb...|Passed ten other ...|2021-07-26 20:51:21|Pretzels, Food, S...|  3.0|     [positive]|               1|\n",
      "|KrVLzXwzm1EgQ8XeZ...|wrgtmWOkq1FwnlSgl...|By far the best b...|2016-02-28 13:52:43|Restaurants, Nigh...| null|     [negative]|               0|\n",
      "|LAlPaYMAWx0ZUNnDY...|ArHq_uM3K7mxAiqwF...|\"Always FANTASTIC...|2015-11-10 16:20:49|Restaurants, Brea...|  5.0|     [positive]|               1|\n",
      "|MN7wg1rpDJTKTkg1j...|2pLIQ0RHSmUbF0eIR...|I've wanted to vi...|2011-06-16 14:59:40|Wine Bars, Bars, ...| null|     [negative]|               0|\n",
      "|Nc6mkga2kKQs9_a92...|DjiBIx1d8USQoSQME...|Located right on ...|2019-12-01 21:52:50|Restaurants, Tex-...| null|     [positive]|               1|\n",
      "|PcdwS2FL3jnpXIwmI...|VQcCL9PiNL_wkGf-u...|Good space upstai...|2016-03-02 19:29:51|American (New), R...|  3.0|     [negative]|               1|\n",
      "|TiiJfsNpPfQ5my6Uq...|3iSRhLUZfZSlLaQw-...|\"Never again.  I ...|2010-04-22 00:35:34|Sushi Bars, Night...|  1.0|     [positive]|               0|\n",
      "|VqHhMjGJI_9bTcgl6...|8ikj9GaC2oEf_leis...|This was so incre...|2016-05-15 16:22:32|Diners, Restauran...|  5.0|     [negative]|               0|\n",
      "|cYTp9j-Rbmpij11dG...|q-zV08jt6U-q05SME...|\"We said \"Hello\" ...|2009-06-17 00:13:56|Sandwiches, Fashi...|  5.0|     [negative]|               0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTableAnalyzedReviews = DeltaTable.forPath(spark, delta_table_path)\n",
    "df = deltaTableAnalyzedReviews.toDF()\n",
    "\n",
    "# Use the DataFrame as desired\n",
    "print(df.count())\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
