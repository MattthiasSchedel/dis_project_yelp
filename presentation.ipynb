{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d34bd907-b8d2-42b6-aa22-4ef4fcf0354a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 570ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d34bd907-b8d2-42b6-aa22-4ef4fcf0354a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/20ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "from delta import *\n",
    "\n",
    "# Import Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Sentiment Analysis - presentation\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .master(\"spark://namenode:7077\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    #.config(\"spark.executor.instances\", \"1\")\\\n",
    "    \n",
    "\n",
    "    \n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, name: string, address: string, city: string, state: string, postal_code: string, latitude: double, longitude: double, stars: double, review_count: int, is_open: int, attributes: string, categories: string, hours: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Dataframes\n",
    "# Review data\n",
    "spark.sparkContext.setJobDescription('load review dataset')\n",
    "review_schema = StructType([StructField(\"review_id\", StringType(), False),\n",
    "      StructField(\"user_id\", StringType(), False),\n",
    "      StructField(\"business_id\", StringType(), False),\n",
    "      StructField(\"stars\", StringType(), False), \n",
    "      StructField(\"useful\", IntegerType(), False),\n",
    "      StructField(\"funny\", IntegerType(), False),\n",
    "      StructField(\"cool\", IntegerType(), False),\n",
    "      StructField(\"text\", StringType(), False),\n",
    "      StructField(\"date\", StringType(), False),])\n",
    "# review_df = spark.read.csv(\"hdfs://namenode:9000/project_data/review.csv\", sep = '|', header = False, schema = review_schema)\n",
    "review_df = spark.read.csv(\"hdfs://namenode:9000/project_data/review_small_b.csv\", sep = '|', header = False, schema = review_schema)\n",
    "review_df.createOrReplaceTempView(\"reviews\")\n",
    "review_df.persist()\n",
    "\n",
    "#review_df.show()\n",
    "\n",
    "# Business data\n",
    "spark.sparkContext.setJobDescription('load review dataset')\n",
    "business_schema = StructType([\n",
    "    StructField(\"business_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"stars\", DoubleType(), True),\n",
    "    StructField(\"review_count\", IntegerType(), True),\n",
    "    StructField(\"is_open\", IntegerType(), True),\n",
    "    StructField(\"attributes\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"hours\", StringType(), True)\n",
    "])\n",
    "business_df = spark.read.csv(\"hdfs://namenode:9000/project_data/business.csv\", sep = '|', header = False, schema = business_schema)\n",
    "business_df.createOrReplaceTempView(\"business\")\n",
    "business_df.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimisation\n",
    "### Combine and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|ApYU3mkJPVqpsrDd5...|LdECsE8lJS7v5GTFT...|We are here.and l...|2018-09-12 00:09:57|Seafood, Restaurants|  5.0|\n",
      "|MN7wg1rpDJTKTkg1j...|2pLIQ0RHSmUbF0eIR...|I've wanted to vi...|2011-06-16 14:59:40|Wine Bars, Bars, ...|  4.0|\n",
      "|gGepmRT-EenDRFiGf...|deWhFgwgU3F11jYMJ...|Okay for what it ...|2017-08-24 23:13:27|Seafood, Salad, R...|  2.0|\n",
      "|z5i8OtKd4L4Kuyvu9...|PP3BBaVxZLcJU54uP...|5stars.          ...|2015-08-16 03:23:35|Italian, American...|  5.0|\n",
      "|-KOxrHhTSayEk01XH...|CrP6JWXBmf_HyMnZJ...|Good place to eat...|2013-02-20 17:34:12|Restaurants, Burg...|  4.0|\n",
      "|4okNZR_Z3whwioH3i...|BxfvdHqETU8jWYUjx...|I love shake shac...|2012-08-05 12:19:57|Food, Hot Dogs, B...|  4.0|\n",
      "|Nc6mkga2kKQs9_a92...|DjiBIx1d8USQoSQME...|Located right on ...|2019-12-01 21:52:50|Restaurants, Tex-...|  5.0|\n",
      "|DhxtQTbPgjg5DIGDX...|pym7c6ZFEtmoH16xN...|Awesome service a...|2011-12-14 21:53:19|Restaurants, Bars...|  5.0|\n",
      "|kXbVRmYjaUIBI8jLs...|iHdrLu8deF5GceB0V...|\"My new favorite ...|2017-12-23 02:58:08|American (Traditi...|  5.0|\n",
      "|4F0rrBlJ2GIbWcNNW...|UakVMT3xrpbFB2pHd...|Great breakfast b...|2018-12-24 18:49:00|Bars, Nightlife, ...|  4.0|\n",
      "|0oCbH-K0tFa0Exbjl...|UakVMT3xrpbFB2pHd...|Well I finally ha...|2018-06-01 22:47:56|Bars, Nightlife, ...|  5.0|\n",
      "|BXVm_qLhx2h0F1Cf-...|Oun4NN-u5yiHIxDqt...|Donuts and fried ...|2013-11-07 23:50:33|Restaurants, Kore...|  5.0|\n",
      "|gehQuizlanQ0852LQ...|L05ImY9-Qga2ri7FK...|The food is good ...|2020-12-25 23:40:50|Sushi Bars, Resta...|  2.0|\n",
      "|LAlPaYMAWx0ZUNnDY...|ArHq_uM3K7mxAiqwF...|\"Always FANTASTIC...|2015-11-10 16:20:49|Restaurants, Brea...|  5.0|\n",
      "|2ecMImprVq9UhB6Pe...|iUZEGx29miZObLd6_...|Wow! First off, I...|2009-11-20 16:54:17|Breakfast & Brunc...|  5.0|\n",
      "|GfbZu-2hg6WzG5b8v...|sJ7RwRN1Pd602KDXb...|Passed ten other ...|2021-07-26 20:51:21|Pretzels, Food, S...|  3.0|\n",
      "|4pw-OuNdrzzZx1NDa...|-CbBGlrmddJsaruk6...|The food isn't go...|2019-10-13 01:52:33|Salad, Restaurant...|  2.0|\n",
      "|vgTzh0vAScdRfbZf4...|_393npnr0Dw1aGKy8...|Brunch was great....|2016-03-06 17:20:56|Restaurants, Pizz...|  5.0|\n",
      "|E0m3ni49ruwkccjZe...|pN3O2ZLRiSLPyCWCe...|\"America used to ...|2010-09-27 04:29:02|Restaurants, Amer...|  1.0|\n",
      "|cYTp9j-Rbmpij11dG...|q-zV08jt6U-q05SME...|\"We said \"Hello\" ...|2009-06-17 00:13:56|Sandwiches, Fashi...|  5.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine data on common restaurant id\n",
    "spark.sparkContext.setJobGroup(\"combine datasets\", \"combine datasets\")\n",
    "reviews_with_category = spark.sql(\"SELECT r.review_id, b.business_id, r.text, r.date, b.categories, r.stars FROM reviews AS r LEFT JOIN business AS b ON b.business_id = r.business_id \")\n",
    "\n",
    "# Filter for only restaurant reviews\n",
    "spark.sparkContext.setJobGroup(\"filter reviews\", \"filter reviews\")\n",
    "restaurant_reviews = reviews_with_category.where(F.col('categories').contains(\"Restaurants\"))\n",
    "restaurant_reviews.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Combine\n",
    "We expect this to be faster since the restaurant dataframe is first made smaller.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|           review_id|         business_id|                text|               date|          categories|stars|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "|z5i8OtKd4L4Kuyvu9...|PP3BBaVxZLcJU54uP...|5stars.          ...|2015-08-16 03:23:35|Italian, American...|  5.0|\n",
      "|kXbVRmYjaUIBI8jLs...|iHdrLu8deF5GceB0V...|\"My new favorite ...|2017-12-23 02:58:08|American (Traditi...|  5.0|\n",
      "|E0m3ni49ruwkccjZe...|pN3O2ZLRiSLPyCWCe...|\"America used to ...|2010-09-27 04:29:02|Restaurants, Amer...|  1.0|\n",
      "|DhxtQTbPgjg5DIGDX...|pym7c6ZFEtmoH16xN...|Awesome service a...|2011-12-14 21:53:19|Restaurants, Bars...|  5.0|\n",
      "|4okNZR_Z3whwioH3i...|BxfvdHqETU8jWYUjx...|I love shake shac...|2012-08-05 12:19:57|Food, Hot Dogs, B...|  4.0|\n",
      "|wcNjtKfLSubxPp8kc...|MZ9ucV1npggMOYY9f...|I recently purcha...|2019-09-01 03:55:28|Home Services, Co...|  5.0|\n",
      "|0oCbH-K0tFa0Exbjl...|UakVMT3xrpbFB2pHd...|Well I finally ha...|2018-06-01 22:47:56|Bars, Nightlife, ...|  5.0|\n",
      "|4F0rrBlJ2GIbWcNNW...|UakVMT3xrpbFB2pHd...|Great breakfast b...|2018-12-24 18:49:00|Bars, Nightlife, ...|  4.0|\n",
      "|4pw-OuNdrzzZx1NDa...|-CbBGlrmddJsaruk6...|The food isn't go...|2019-10-13 01:52:33|Salad, Restaurant...|  2.0|\n",
      "|wn1s1lPOuARSuMdo5...|7jSm5z3n2JfDHsFAX...|Live around the c...|2015-08-15 20:24:27|Signmaking, Art G...|  1.0|\n",
      "|ezwqc_Dj0FbnbJKjQ...|9tOw202PkO9efW8zS...|\"I normally do no...|2011-12-01 03:37:00|  Pizza, Restaurants|  1.0|\n",
      "|AZTGIy-E97N3qSlm7...|LaQgujKtAQ4yPSmqR...|Great burgers and...|2017-10-27 22:41:00|American (Traditi...|  5.0|\n",
      "|UQvcvXb6IZsXJxNhZ...|P3EKMH2N1d5SbP1QI...|Does a great job ...|2019-10-10 02:56:50|Beauty & Spas, Ha...|  5.0|\n",
      "|psA_ANJ6pEyD9MS_A...|fIMPvGtvkrfT2zHyf...|If YELP posted ne...|2018-05-23 20:29:39|Shopping, Indoor ...|  1.0|\n",
      "|2ecMImprVq9UhB6Pe...|iUZEGx29miZObLd6_...|Wow! First off, I...|2009-11-20 16:54:17|Breakfast & Brunc...|  5.0|\n",
      "|GfbZu-2hg6WzG5b8v...|sJ7RwRN1Pd602KDXb...|Passed ten other ...|2021-07-26 20:51:21|Pretzels, Food, S...|  3.0|\n",
      "|JzQmAnl57t0zzAujo...|2bfExBV5NEmn79wUw...|\"This is a fun ad...|2013-03-18 00:33:53|Food, Ice Cream &...|  4.0|\n",
      "|MN7wg1rpDJTKTkg1j...|2pLIQ0RHSmUbF0eIR...|I've wanted to vi...|2011-06-16 14:59:40|Wine Bars, Bars, ...|  4.0|\n",
      "|LAlPaYMAWx0ZUNnDY...|ArHq_uM3K7mxAiqwF...|\"Always FANTASTIC...|2015-11-10 16:20:49|Restaurants, Brea...|  5.0|\n",
      "|-KOxrHhTSayEk01XH...|CrP6JWXBmf_HyMnZJ...|Good place to eat...|2013-02-20 17:34:12|Restaurants, Burg...|  4.0|\n",
      "+--------------------+--------------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter for only restaurant reviews\n",
    "spark.sparkContext.setJobDescription('filter reviews')\n",
    "business_df = business_df.where(F.col('categories').contains(\"Restaurants\"))\n",
    "\n",
    "\n",
    "# Combine data on common restaurant id\n",
    "spark.sparkContext.setJobDescription('combine datasets')\n",
    "restaurant_reviews = spark.sql(\"SELECT r.review_id, b.business_id, r.text, r.date, b.categories, r.stars FROM reviews AS r LEFT JOIN business AS b ON b.business_id = r.business_id \")\n",
    "\n",
    "restaurant_reviews.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('pipeline builder')\n",
    "# Build a pipeline\n",
    "# Document Assembler\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -tokenizer')\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")#\\\n",
    "    #.fit(restaurant_reviews) # apperantly works without this\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -normalizer')\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normal\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -model')\n",
    "vivekn = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"normal\"]) \\\n",
    "    .setOutputCol(\"result_sentiment\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -model=finish')\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"result_sentiment\"]) \\\n",
    "    .setOutputCols(\"final_sentiment\")\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline executor')\n",
    "pipeline = Pipeline().setStages([documentAssembler, tokenizer, normalizer, vivekn, finisher])\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline fit')\n",
    "pipelineModel = pipeline.fit(restaurant_reviews)\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -pipeline transform')\n",
    "# Calculate sentiment for all restaurant reviews\n",
    "result = pipelineModel.transform(restaurant_reviews)\n",
    "\n",
    "spark.sparkContext.setJobDescription('pipeline -result check')\n",
    "# Check accuracy\n",
    "result = result.withColumn(\"right_prediction\", \n",
    "                   F.when(((F.array_contains(F.col(\"final_sentiment\"),\"positive\")) & (F.col(\"stars\").isin([\"5.0\", \"4.0\", \"3.0\"]))) |\n",
    "                        ((F.array_contains(F.col(\"final_sentiment\"),\"negative\")) & (F.col(\"stars\").isin([\"3.0\", \"2.0\", \"1.0\"]))), \n",
    "                        1).otherwise(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding Accuracy without persist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription('count right ones')\n",
    "count_ones = result.agg(F.sum(\"right_prediction\")).collect()[0][0]\n",
    "\n",
    "total_count = result.count()\n",
    "\n",
    "print(f\"Prediction accuracy for sentiment: {count_ones/total_count}\")\n",
    "result.write.format(\"delta\").mode(\"overwrite\").save(\"/temp/sentiment_predicted_restaurant_reviews\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding accuracy with persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.persist()\n",
    "\n",
    "spark.sparkContext.setJobDescription('count right ones')\n",
    "#### TO-DO --- try to optomise below line.\n",
    "count_ones = result.agg(F.sum(\"right_prediction\")).collect()[0][0]\n",
    "result.write.format(\"delta\").mode(\"overwrite\").save(\"/temp/sentiment_predicted_restaurant_reviews\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upsert \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_table_path = \"/temp/sentiment_predicted_restaurant_reviews\"\n",
    "delta_table_path = \"/temp/upsert_presentation\"\n",
    "# Check if the Delta table exists\n",
    "if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "    print(\"Updating delta\")\n",
    "    # If the Delta table exists, load it into a DataFrame\n",
    "    deltaTableAnalyzedReviews = DeltaTable.forPath(spark, delta_table_path)\n",
    "    deltaTableAnalyzedReviews.alias('old') \\\n",
    "        .merge(\n",
    "        result.alias('updates'),\n",
    "        'old.review_id = updates.review_id'\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"review_id\": \"updates.review_id\",\n",
    "      \"business_id\": \"updates.business_id\",\n",
    "      \"text\": \"updates.text\",\n",
    "      \"date\": \"updates.date\",\n",
    "      \"categories\": \"updates.categories\",\n",
    "      \"final_sentiment\": \"updates.final_sentiment\",\n",
    "    }\n",
    "  ) \\\n",
    "  .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"review_id\": \"updates.review_id\",\n",
    "      \"business_id\": \"updates.business_id\",\n",
    "      \"text\": \"updates.text\",\n",
    "      \"date\": \"updates.date\",\n",
    "      \"categories\": \"updates.categories\",\n",
    "      \"final_sentiment\": \"updates.final_sentiment\",\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n",
    "else:\n",
    "    # If the Delta table does not exist, create it\n",
    "    print(\"Creating delta\")\n",
    "    deltaTableAnalyzedReviews = result.write.format('delta').mode('overwrite').save(delta_table_path)\n",
    "    deltaTableAnalyzedReviews = DeltaTable.forPath(spark, delta_table_path)\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
